{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20d82df6",
   "metadata": {},
   "source": [
    "# ETL Pipeline Overview\n",
    "\n",
    "**Last Updated:** January 2026\n",
    "\n",
    "This notebook provides a comprehensive overview of the Chinese Archives ETL pipeline, including:\n",
    "\n",
    "1. **Pipeline Architecture** - 4-step workflow from PDF upload to structured parsing\n",
    "2. **Job Tracking System** - How jobs are tracked and linked\n",
    "3. **Standardized Interface** - Common patterns across all steps\n",
    "4. **Run Modes** - Understanding regular runs, resume, and force rerun\n",
    "5. **Advanced Features** - Wildcards and dry-run previews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76045b8b",
   "metadata": {},
   "source": [
    "## 1. Pipeline Architecture\n",
    "\n",
    "The ETL pipeline consists of 4 sequential steps:\n",
    "\n",
    "```\n",
    "Step 0: upload_pdfs       ‚Üí Upload scanned PDFs to B2 cloud storage\n",
    "         ‚Üì (generates job_id)\n",
    "         \n",
    "Step 2: run_ocr          ‚Üí Send PDFs to Runpod API for OCR processing\n",
    "         ‚Üì (references upload job_id)\n",
    "         \n",
    "Step 3: sync_ocr         ‚Üí Download OCR results from Runpod to B2\n",
    "         ‚Üì (references ocr job_id)\n",
    "         \n",
    "Step 4: parse_structure  ‚Üí Detect pagination and extract table of contents\n",
    "         ‚Üì (references sync job_id)\n",
    "```\n",
    "\n",
    "**Key Principle:** Each step references only the **immediate prior step's job_id** (via `--source-job-id`), creating a clear lineage chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bd6716",
   "metadata": {},
   "source": [
    "## 2. Job Tracking System\n",
    "\n",
    "Each pipeline run creates a unique **job_id** with timestamp format: `YYYY-MM-DD_HH-MM-SS`\n",
    "\n",
    "### Data Storage Structure\n",
    "\n",
    "```\n",
    "data/\n",
    "‚îú‚îÄ‚îÄ sources/                          # Upload metadata only\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ job_metadata/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ 2026-01-02_00-28-28.json\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ analytics/                        # All analytics steps\n",
    "    ‚îú‚îÄ‚îÄ job_registry/                 # Central tracking\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ 2026-01-02_00-33-11.json  # OCR job\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ 2026-01-02_11-32-58.json  # Parse job\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ ocr/                          # Step-specific results\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ job_metadata/\n",
    "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 2026-01-02_00-33-11.json\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ dagz_v01/\n",
    "    ‚îÇ       ‚îú‚îÄ‚îÄ latest -> 2026-01-02_00-33-11/\n",
    "    ‚îÇ       ‚îî‚îÄ‚îÄ 2026-01-02_00-33-11/\n",
    "    ‚îÇ           ‚îî‚îÄ‚îÄ dagz_v01.json\n",
    "    ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ parse_structure/\n",
    "        ‚îî‚îÄ‚îÄ ... (same pattern)\n",
    "```\n",
    "\n",
    "### Metadata Tracking\n",
    "\n",
    "Each job maintains **dual metadata**:\n",
    "\n",
    "1. **Task-specific metadata** (`data/analytics/{step}/job_metadata/{job_id}.json`)\n",
    "   - Detailed per-citekey results\n",
    "   - Processing statistics\n",
    "   - Error messages\n",
    "\n",
    "2. **Central registry** (`data/analytics/job_registry/{job_id}.json`)\n",
    "   - High-level job info\n",
    "   - Step name and timestamp\n",
    "   - Source job linkage\n",
    "   - Overall status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a109dba",
   "metadata": {},
   "source": [
    "## 3. Standardized Interface\n",
    "\n",
    "All pipeline steps (except `upload_pdfs`) follow the same input pattern:\n",
    "\n",
    "### Input Selection (mutually exclusive)\n",
    "\n",
    "```bash\n",
    "--input FILE          # Read citekeys from YAML file\n",
    "--citekeys CK1 CK2... # Explicit list via command line\n",
    "--resume-from JOB_ID  # Resume failed items from previous job\n",
    "```\n",
    "\n",
    "### Required Arguments\n",
    "\n",
    "```bash\n",
    "--source-job-id JOB_ID  # Explicit reference to prior step's job\n",
    "```\n",
    "\n",
    "**Important:** Never use `\"latest\"` as a job ID. Always use explicit timestamps.\n",
    "\n",
    "### Optional Flags\n",
    "\n",
    "```bash\n",
    "--force-rerun    # Reprocess all citekeys (ignore existing results)\n",
    "--dry-run        # Preview what would be processed without execution\n",
    "```\n",
    "\n",
    "### Example Commands\n",
    "\n",
    "```bash\n",
    "# Step 2: OCR (after upload)\n",
    "python scripts/run_ocr.py \\\n",
    "    --source-job-id 2026-01-02_00-28-28 \\\n",
    "    --citekeys dagz_v01 dagz_v02\n",
    "\n",
    "# Step 3: Sync (after OCR)\n",
    "python scripts/sync_ocr.py \\\n",
    "    --source-job-id 2026-01-02_00-33-11 \\\n",
    "    --input config/my_citekeys.yaml\n",
    "\n",
    "# Step 4: Parse (after sync)\n",
    "python scripts/parse_structure.py \\\n",
    "    --source-job-id 2026-01-02_11-15-30 \\\n",
    "    --resume-from 2026-01-02_11-32-58\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c50793",
   "metadata": {},
   "source": [
    "## 4. Run Modes: Understanding Runs and Reruns\n",
    "\n",
    "### The Data Worker Scenario\n",
    "\n",
    "Imagine you're a data worker processing 100 documents through OCR:\n",
    "\n",
    "**Monday Morning (First Run)**\n",
    "```bash\n",
    "python scripts/run_ocr.py \\\n",
    "    --source-job-id 2026-01-02_00-28-28 \\\n",
    "    --input config/all_docs.yaml\n",
    "```\n",
    "- Creates job: `2026-01-02_09-00-00`\n",
    "- Processes: All 100 documents\n",
    "- Result: 95 succeed, 5 fail due to API timeouts\n",
    "\n",
    "**Tuesday Morning (Resume Failed)**\n",
    "```bash\n",
    "python scripts/run_ocr.py \\\n",
    "    --source-job-id 2026-01-02_00-28-28 \\\n",
    "    --resume-from 2026-01-02_09-00-00\n",
    "```\n",
    "- **Reuses same job ID:** `2026-01-02_09-00-00`\n",
    "- Processes: Only the 5 failed documents\n",
    "- Result: All 5 succeed\n",
    "- Total: All 100 documents now complete in job `2026-01-02_09-00-00`\n",
    "\n",
    "**Wednesday (Change OCR Parameters - Force Rerun)**\n",
    "\n",
    "You realize the OCR confidence threshold was too low. You need to reprocess with better settings.\n",
    "\n",
    "```bash\n",
    "python scripts/run_ocr.py \\\n",
    "    --source-job-id 2026-01-02_00-28-28 \\\n",
    "    --input config/all_docs.yaml \\\n",
    "    --force-rerun\n",
    "```\n",
    "- **Creates NEW job ID:** `2026-01-03_10-30-00`\n",
    "- Processes: All 100 documents again (even though they already have results)\n",
    "- Result: New results with improved OCR parameters\n",
    "- History: Old results still available in `2026-01-02_09-00-00`\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Mode | Job ID | Processes | Use Case |\n",
    "|------|--------|-----------|----------|\n",
    "| **First Run** | Creates new | All citekeys | Initial processing |\n",
    "| **Regular Rerun** | Creates new if needed | Only missing results | Continue from where you left off |\n",
    "| **Resume** | Reuses existing | Only failed items | Fix failures without new job |\n",
    "| **Force Rerun** | Always creates new | ALL citekeys | Changed parameters, need fresh results |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2df2e90",
   "metadata": {},
   "source": [
    "### Decision Tree: Which Mode to Use?\n",
    "\n",
    "```\n",
    "Do you have results from a previous run?\n",
    "‚îÇ\n",
    "‚îú‚îÄ NO ‚Üí Use regular run (--input or --citekeys)\n",
    "‚îÇ        Creates new job, processes all citekeys\n",
    "‚îÇ\n",
    "‚îî‚îÄ YES ‚Üí Did the previous run have failures?\n",
    "         ‚îÇ\n",
    "         ‚îú‚îÄ NO ‚Üí Did you change parameters/config?\n",
    "         ‚îÇ        ‚îÇ\n",
    "         ‚îÇ        ‚îú‚îÄ NO ‚Üí Nothing to do! Results already exist.\n",
    "         ‚îÇ        ‚îÇ\n",
    "         ‚îÇ        ‚îî‚îÄ YES ‚Üí Use --force-rerun\n",
    "         ‚îÇ                 Creates new job, reprocesses everything\n",
    "         ‚îÇ\n",
    "         ‚îî‚îÄ YES ‚Üí Do you want to create a new job?\n",
    "                  ‚îÇ\n",
    "                  ‚îú‚îÄ NO ‚Üí Use --resume-from\n",
    "                  ‚îÇ        Reuses job ID, processes only failures\n",
    "                  ‚îÇ\n",
    "                  ‚îî‚îÄ YES ‚Üí Use regular run (--input or --citekeys)\n",
    "                           Creates new job, processes missing results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ab7ec",
   "metadata": {},
   "source": [
    "## 5. Advanced Features\n",
    "\n",
    "### Wildcard Patterns\n",
    "\n",
    "Use `_v*` and `_y*` wildcards to match multiple citekeys:\n",
    "\n",
    "```bash\n",
    "# Process all volumes of DAGZ\n",
    "python scripts/parse_structure.py \\\n",
    "    --source-job-id 2026-01-02_11-15-30 \\\n",
    "    --citekeys \"dagz_v*\"\n",
    "    \n",
    "# Matches: dagz_v01, dagz_v02, dagz_v03, ...\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Process all years 2000-2009 for MZDNP\n",
    "python scripts/parse_structure.py \\\n",
    "    --source-job-id 2026-01-02_11-15-30 \\\n",
    "    --citekeys \"mzdnp_y200*\"\n",
    "    \n",
    "# Matches: mzdnp_y2001, mzdnp_y2002, ..., mzdnp_y2009\n",
    "```\n",
    "\n",
    "**Why only _v* and _y*?**\n",
    "- Prevents overly broad matches (e.g., `dagz*` matching everything)\n",
    "- Matches common naming patterns in our corpus\n",
    "- Reduces risk of accidental batch processing\n",
    "\n",
    "### Dry Run Preview\n",
    "\n",
    "Preview what would be processed before committing:\n",
    "\n",
    "```bash\n",
    "python scripts/parse_structure.py \\\n",
    "    --source-job-id 2026-01-02_11-15-30 \\\n",
    "    --citekeys \"dagz_v*\" \\\n",
    "    --dry-run\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "======================================================================\n",
    "üîç DRY RUN PREVIEW: parse_structure\n",
    "======================================================================\n",
    "\n",
    "üìã Configuration:\n",
    "  Source Job ID:    2026-01-02_11-15-30\n",
    "  Force Rerun:      False\n",
    "  Total Citekeys:   5\n",
    "\n",
    "‚úÖ Citekeys to Process (2):\n",
    "    ‚Ä¢ dagz_v04\n",
    "    ‚Ä¢ dagz_v05\n",
    "\n",
    "‚äò Citekeys to Skip (3):\n",
    "    ‚Ä¢ dagz_v01: Result exists in job 2026-01-02_11-32-58\n",
    "    ‚Ä¢ dagz_v02: Result exists in job 2026-01-02_11-32-58\n",
    "    ‚Ä¢ dagz_v03: Result exists in job 2026-01-02_11-32-58\n",
    "\n",
    "üÜî Estimated Job ID:\n",
    "  2026-01-02_15-30-00\n",
    "\n",
    "‚úÖ Will create new job and process 2 citekeys\n",
    "\n",
    "======================================================================\n",
    "üí° To execute: Remove --dry-run flag\n",
    "======================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521118bf",
   "metadata": {},
   "source": [
    "## 6. Complete Example Workflow\n",
    "\n",
    "Let's walk through a complete pipeline run:\n",
    "\n",
    "### Step 0: Upload PDFs\n",
    "\n",
    "```bash\n",
    "python scripts/upload_pdfs.py \\\n",
    "    --input config/my_docs.yaml\n",
    "```\n",
    "**Result:** Job `2026-01-02_00-28-28` created with 10 documents uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebda930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "import sys\n",
    "sys.path.insert(0, '../../scripts')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Show directory structure\n",
    "analytics_root = Path('../../data/analytics')\n",
    "print(\"üìÅ Data Structure:\")\n",
    "print(f\"Analytics Root: {analytics_root}\")\n",
    "print(f\"Exists: {analytics_root.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed44546",
   "metadata": {},
   "source": [
    "### Step 2: Run OCR\n",
    "\n",
    "```bash\n",
    "# Preview first\n",
    "python scripts/run_ocr.py \\\n",
    "    --source-job-id 2026-01-02_00-28-28 \\\n",
    "    --citekeys \"dagz_v*\" \\\n",
    "    --dry-run\n",
    "\n",
    "# Execute\n",
    "python scripts/run_ocr.py \\\n",
    "    --source-job-id 2026-01-02_00-28-28 \\\n",
    "    --citekeys dagz_v01 dagz_v02 dagz_v03\n",
    "```\n",
    "**Result:** Job `2026-01-02_00-33-11` created, 3 documents processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f695c16c",
   "metadata": {},
   "source": [
    "### Step 3: Sync OCR Results\n",
    "\n",
    "```bash\n",
    "python scripts/sync_ocr.py \\\n",
    "    --source-job-id 2026-01-02_00-33-11 \\\n",
    "    --citekeys dagz_v01 dagz_v02 dagz_v03\n",
    "```\n",
    "**Result:** Job `2026-01-02_11-15-30` created, 3 documents synced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe52c38",
   "metadata": {},
   "source": [
    "### Step 4: Parse Structure\n",
    "\n",
    "```bash\n",
    "python scripts/parse_structure.py \\\n",
    "    --source-job-id 2026-01-02_11-15-30 \\\n",
    "    --citekeys dagz_v01 dagz_v02 dagz_v03\n",
    "```\n",
    "**Result:** Job `2026-01-02_11-32-58` created, pagination detected for 3 documents\n",
    "\n",
    "### Handling Failures\n",
    "\n",
    "Suppose `dagz_v03` failed. Resume it:\n",
    "\n",
    "```bash\n",
    "python scripts/parse_structure.py \\\n",
    "    --source-job-id 2026-01-02_11-15-30 \\\n",
    "    --resume-from 2026-01-02_11-32-58\n",
    "```\n",
    "**Result:** Same job `2026-01-02_11-32-58` updated, `dagz_v03` now complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd07d549",
   "metadata": {},
   "source": [
    "## 7. Best Practices\n",
    "\n",
    "### ‚úÖ DO\n",
    "\n",
    "- **Use explicit job IDs:** Always specify `--source-job-id` with exact timestamp\n",
    "- **Preview first:** Use `--dry-run` for large batches to verify what will be processed\n",
    "- **Use wildcards wisely:** Leverage `_v*` and `_y*` for volume and year ranges\n",
    "- **Resume failures:** Use `--resume-from` to fix issues in the same job\n",
    "- **Force rerun when needed:** Use `--force-rerun` when parameters change\n",
    "\n",
    "### ‚ùå DON'T\n",
    "\n",
    "- **Use \"latest\":** Never use \"latest\" as job ID (breaks reproducibility)\n",
    "- **Skip source tracking:** Always provide `--source-job-id` to maintain lineage\n",
    "- **Ignore dry run:** Don't skip `--dry-run` for large or complex operations\n",
    "- **Use broad wildcards:** Don't use patterns like `*` or `dagz*` (only `_v*` and `_y*`)\n",
    "- **Mix input modes:** Don't combine `--input`, `--citekeys`, and `--resume-from`\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Lost lineage:** Running a step without `--source-job-id` breaks the chain\n",
    "2. **Accidental reprocessing:** Not checking existing results wastes compute time\n",
    "3. **Wildcard explosions:** Using overly broad patterns processes unintended files\n",
    "4. **Resume confusion:** Using `--resume-from` with wrong source creates mismatched data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
