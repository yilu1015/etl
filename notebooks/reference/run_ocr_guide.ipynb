{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae9d8b1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### 1. Runpod Endpoint\n",
    "\n",
    "You must have an active Runpod serverless endpoint with PaddleOCR-VL handler deployed.\n",
    "\n",
    "Set these environment variables:\n",
    "\n",
    "```bash\n",
    "export RUNPOD_API_KEY=\"your_runpod_api_key\"\n",
    "export RUNPOD_ENDPOINT_ID=\"4nq7w0tw2k8uwk\"  # Your endpoint ID\n",
    "```\n",
    "\n",
    "Or in `.env` file:\n",
    "\n",
    "```\n",
    "RUNPOD_API_KEY=your_runpod_api_key\n",
    "RUNPOD_ENDPOINT_ID=4nq7w0tw2k8uwk\n",
    "```\n",
    "\n",
    "### 2. B2 Credentials (Source)\n",
    "\n",
    "Credentials for reading PDFs from B2:\n",
    "\n",
    "```bash\n",
    "export B2_SOURCE_ACCESS_KEY_ID=\"your_b2_key_id\"\n",
    "export B2_SOURCE_SECRET_ACCESS_KEY=\"your_b2_secret\"\n",
    "```\n",
    "\n",
    "### 3. Optional: Runpod S3 Credentials (Image Extraction)\n",
    "\n",
    "Only needed if using `--extract-images` to download visualization images:\n",
    "\n",
    "```bash\n",
    "export RUNPOD_S3_ACCESS_KEY_ID=\"user_xxxxx\"\n",
    "export RUNPOD_S3_SECRET_ACCESS_KEY=\"rps_xxxxx\"\n",
    "export RUNPOD_S3_ENDPOINT_URL=\"https://s3api-eu-ro-1.runpod.io\"\n",
    "export RUNPOD_S3_REGION=\"EU-RO-1\"\n",
    "export RUNPOD_NETWORK_VOLUME_ID=\"u2qv0e9yfd\"\n",
    "```\n",
    "\n",
    "### 4. Configuration\n",
    "\n",
    "Configuration is loaded from `config/ocr_config.yaml`. Override specific settings via CLI args or environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489de8e6",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "### Process a single citekey\n",
    "\n",
    "```bash\n",
    "uv run scripts/run_ocr.py --citekeys test_v04 --output data/analytics/ocr\n",
    "```\n",
    "\n",
    "### Process multiple citekeys\n",
    "\n",
    "```bash\n",
    "uv run scripts/run_ocr.py --citekeys dagz_v01 dagz_v02 dagz_v03 --output data/analytics/ocr\n",
    "```\n",
    "\n",
    "### Process from local directory\n",
    "\n",
    "```bash\n",
    "uv run scripts/run_ocr.py --input data/sources/test/ --output data/analytics/ocr\n",
    "```\n",
    "\n",
    "### Process single local PDF file\n",
    "\n",
    "```bash\n",
    "uv run scripts/run_ocr.py --input tests/fixtures/sample_pdfs/test_v04.pdf --output data/analytics/ocr\n",
    "```\n",
    "\n",
    "### With image extraction\n",
    "\n",
    "```bash\n",
    "uv run scripts/run_ocr.py --citekeys dagz_v01 --output data/analytics/ocr --extract-images\n",
    "```\n",
    "\n",
    "### Custom batch size (pages per batch)\n",
    "\n",
    "```bash\n",
    "uv run scripts/run_ocr.py --citekeys dagz_v01 dagz_v02 --output data/analytics/ocr --batch-pages 200\n",
    "```\n",
    "\n",
    "### Explicit source job ID (for lineage)\n",
    "\n",
    "```bash\n",
    "uv run scripts/run_ocr.py --citekeys dagz_v01 dagz_v02 --output data/analytics/ocr --source-job-id 2026-01-01_20-48-14\n",
    "```\n",
    "\n",
    "### Auto-detect latest source job\n",
    "\n",
    "```bash\n",
    "uv run scripts/run_ocr.py --citekeys dagz_v01 dagz_v02 --output data/analytics/ocr --source-job-id latest\n",
    "```\n",
    "\n",
    "### Quiet mode (suppress output)\n",
    "\n",
    "```bash\n",
    "uv run scripts/run_ocr.py --citekeys dagz_v01 --output data/analytics/ocr --quiet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeaaebe",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "### Step 1: Discover & Validate Citekeys\n",
    "\n",
    "The script accepts input in two ways:\n",
    "\n",
    "**Option A: Explicit citekeys (for B2 source)**\n",
    "```bash\n",
    "run_ocr.py --citekeys dagz_v01 dagz_v02 dagz_v03\n",
    "```\n",
    "\n",
    "**Option B: Local files (discover citekeys from filenames)**\n",
    "```bash\n",
    "run_ocr.py --input data/sources/test/  # Discovers test_v01.pdf, test_v02.pdf, etc.\n",
    "```\n",
    "\n",
    "All citekeys are validated against B2 to ensure they were uploaded first via `upload_pdfs.py`.\n",
    "\n",
    "### Step 2: Auto-detect Source Job IDs (Lineage)\n",
    "\n",
    "**Default behavior** (if no `--source-job-id` specified):\n",
    "- Search all job metadata files in `data/sources/job_metadata/`\n",
    "- For each citekey, find which upload job contains it\n",
    "- Only trust jobs with `status=\"completed\"` (same safety as upload_pdfs.py)\n",
    "- Store per-citekey mapping for later\n",
    "\n",
    "**Example output:**\n",
    "```\n",
    "ðŸ” Auto-detecting source job IDs from citekey metadata...\n",
    "  dagz_v01: 2026-01-01_20-48-14\n",
    "  dagz_v02: 2026-01-01_20-48-14\n",
    "  dagz_v03: 2026-01-02_14-30-22\n",
    "  Lineage: 3/3 citekey(s) mapped to source job(s)\n",
    "```\n",
    "\n",
    "**Why lineage matters:**\n",
    "- Reproducibility: \"Which exact PDF batch produced this OCR?\"\n",
    "- Debugging: \"If OCR output is wrong, check the source PDF\"\n",
    "- Audit trail: Complete history from source â†’ upload â†’ OCR\n",
    "\n",
    "### Step 3: Get File Sizes & Page Counts\n",
    "\n",
    "**For local files** (from `--input`):\n",
    "- Reads file size directly from disk\n",
    "- Uses PyPDF2 to get actual page count (if available)\n",
    "- Falls back to estimation if PyPDF2 not available\n",
    "\n",
    "**For B2 files** (from `--citekeys`):\n",
    "- Queries B2 metadata for file size\n",
    "- Downloads temp copy to count pages with PyPDF2\n",
    "- Deletes temp file after counting\n",
    "\n",
    "### Step 4: Create Batches\n",
    "\n",
    "Groups PDFs into batches based on **page count** (not file count):\n",
    "\n",
    "```\n",
    "ðŸ“¦ Creating batches (target 300 pages per batch)...\n",
    "   3 batch(es):\n",
    "     Batch 1:  2 file(s),      100MB (  372 pages)\n",
    "     Batch 2:  1 file(s),       50MB (  280 pages)\n",
    "     Batch 3:  1 file(s),       40MB (  195 pages)\n",
    "```\n",
    "\n",
    "**Why page-based batching?**\n",
    "- GPU memory is proportional to pages, not file count\n",
    "- Balanced batches improve throughput\n",
    "- Respects `--batch-pages` parameter for tuning\n",
    "\n",
    "### Step 5: Generate Presigned URLs\n",
    "\n",
    "For each file, generate a **temporary, limited-access URL** to B2:\n",
    "- Valid for 12 hours (configurable in `ocr_config.yaml`)\n",
    "- No B2 credentials embedded\n",
    "- Unique per file\n",
    "- Handler receives only URLs, not credentials\n",
    "\n",
    "### Step 6: Submit to Runpod\n",
    "\n",
    "For each batch, submit a job with:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"input\": {\n",
    "    \"job_id\": \"2026-01-01_21-45-14\",\n",
    "    \"files\": [\n",
    "      {\n",
    "        \"type\": \"url\",\n",
    "        \"url\": \"https://f000.backblazeb2.com/...\",\n",
    "        \"metadata\": {\n",
    "          \"citekey\": \"dagz_v01\",\n",
    "          \"job_id\": \"2026-01-01_21-45-14\",\n",
    "          \"batch_idx\": 1,\n",
    "          \"source_job_id\": \"2026-01-01_20-48-14\",\n",
    "          \"pipeline_step\": \"ocr\"\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"pipeline_config\": { ... },\n",
    "    \"predict_params\": { ... }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Step 7: Poll for Completion\n",
    "\n",
    "Polls Runpod API with exponential backoff:\n",
    "- Check every 2 seconds (configurable)\n",
    "- Retry on transient errors (DNS, timeout)\n",
    "- Fail fast on permanent errors\n",
    "- Max polling duration: 30 minutes (configurable)\n",
    "\n",
    "### Step 8: Process Results\n",
    "\n",
    "When job completes, Runpod returns OCR results:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"results\": [\n",
    "    {\n",
    "      \"metadata\": {\n",
    "        \"citekey\": \"dagz_v01\",\n",
    "        \"source_job_id\": \"2026-01-01_20-48-14\",\n",
    "        \"job_id\": \"2026-01-01_21-45-14\"\n",
    "      },\n",
    "      \"pages\": [\n",
    "        {\n",
    "          \"page_num\": 1,\n",
    "          \"text\": \"...\",\n",
    "          \"confidence\": 0.92\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Step 9: Save Results & Metadata\n",
    "\n",
    "Results are saved locally:\n",
    "\n",
    "```\n",
    "data/analytics/ocr/\n",
    "  dagz_v01/\n",
    "    2026-01-01_21-45-14/\n",
    "      results.json\n",
    "      images/             (if --extract-images)\n",
    "        page_001.png\n",
    "        page_002.png\n",
    "        ...\n",
    "    latest -> 2026-01-01_21-45-14\n",
    "  job_metadata/\n",
    "    2026-01-01_21-45-14.json\n",
    "    latest.json -> 2026-01-01_21-45-14.json\n",
    "```\n",
    "\n",
    "Metadata includes:\n",
    "- Job ID, timestamp\n",
    "- List of citekeys processed\n",
    "- Batch info (how many batches, which Runpod job IDs)\n",
    "- Source job ID (for lineage)\n",
    "- Pipeline configuration used\n",
    "- Results summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d9c3b",
   "metadata": {},
   "source": [
    "## Common Workflows\n",
    "\n",
    "### Workflow 1: First-time OCR of Uploaded PDFs\n",
    "\n",
    "```bash\n",
    "# Just uploaded PDFs via upload_pdfs.py\n",
    "# Auto-detect source job and process\n",
    "\n",
    "uv run scripts/run_ocr.py \\\n",
    "  --citekeys dagz_v01 dagz_v02 dagz_v03 \\\n",
    "  --output data/analytics/ocr\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "- Citekeys validated against B2\n",
    "- Source job IDs auto-detected from metadata\n",
    "- Batches created based on page counts\n",
    "- Results saved to `data/analytics/ocr/<citekey>/2026-01-01_HH-MM-SS/`\n",
    "- Metadata saved to `data/analytics/ocr/job_metadata/2026-01-01_HH-MM-SS.json`\n",
    "- `latest` symlinks created for easy access\n",
    "\n",
    "### Workflow 2: Test with Local PDF Before Full Run\n",
    "\n",
    "```bash\n",
    "# Test OCR configuration with a small sample\n",
    "\n",
    "uv run scripts/run_ocr.py \\\n",
    "  --input tests/fixtures/sample_pdfs/test_v04.pdf \\\n",
    "  --output data/analytics/ocr\n",
    "\n",
    "# Check results\n",
    "cat data/analytics/ocr/test_v04/latest/results.json | python -m json.tool\n",
    "```\n",
    "\n",
    "### Workflow 3: Large Batch with Custom Page Sizing\n",
    "\n",
    "```bash\n",
    "# Process many PDFs, custom batch size for GPU tuning\n",
    "\n",
    "uv run scripts/run_ocr.py \\\n",
    "  --citekeys dagz_v01 dagz_v02 ... dagz_v50 \\\n",
    "  --output data/analytics/ocr \\\n",
    "  --batch-pages 500  # Larger batches (more GPU memory)\n",
    "```\n",
    "\n",
    "### Workflow 4: Extract Visualizations\n",
    "\n",
    "```bash\n",
    "# Save PNG visualization images of OCR detection\n",
    "\n",
    "uv run scripts/run_ocr.py \\\n",
    "  --citekeys dagz_v01 \\\n",
    "  --output data/analytics/ocr \\\n",
    "  --extract-images\n",
    "\n",
    "# Results include images\n",
    "ls data/analytics/ocr/dagz_v01/latest/images/\n",
    "# Output: page_001.png, page_002.png, ...\n",
    "```\n",
    "\n",
    "### Workflow 5: Explicit Source Job Tracking\n",
    "\n",
    "```bash\n",
    "# Process PDFs from a specific upload batch\n",
    "# Useful when multiple batches were uploaded\n",
    "\n",
    "uv run scripts/run_ocr.py \\\n",
    "  --citekeys dagz_v01 dagz_v02 \\\n",
    "  --output data/analytics/ocr \\\n",
    "  --source-job-id 2025-12-31_22-04-05\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab55941",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "### ocr_config.yaml Structure\n",
    "\n",
    "Configuration is centralized in `config/ocr_config.yaml`. Key sections:\n",
    "\n",
    "**B2 Source** (where PDFs are stored):\n",
    "```yaml\n",
    "b2_source:\n",
    "  bucket: cna-sources\n",
    "  region: us-east-005\n",
    "  endpoint_url: https://s3.us-east-005.backblazeb2.com\n",
    "  presigned_url_expiry: 43200  # 12 hours\n",
    "```\n",
    "\n",
    "**Batching** (how to group PDFs):\n",
    "```yaml\n",
    "batching:\n",
    "  default_pages_per_batch: 300\n",
    "  max_pages_per_batch: 1200\n",
    "```\n",
    "\n",
    "**Job Submission** (Runpod interaction):\n",
    "```yaml\n",
    "job_submission:\n",
    "  poll_interval: 2           # Check every 2 seconds\n",
    "  max_poll_duration: 1800    # Max 30 minutes\n",
    "  max_retries_per_status: 3  # Retry transient errors\n",
    "```\n",
    "\n",
    "**PaddleOCR Pipeline Config**:\n",
    "```yaml\n",
    "pipeline:\n",
    "  ocr:\n",
    "    engine: paddleocr\n",
    "    languages: [ch, en]\n",
    "    gpu_id: 0\n",
    "    batch_size: 32\n",
    "    model_dir: null  # Use default cached models\n",
    "```\n",
    "\n",
    "**PaddleOCR Predict Params** (OCR tuning):\n",
    "```yaml\n",
    "predict:\n",
    "  ocr:\n",
    "    det_db_thresh: 0.3\n",
    "    rec_batch_num: 15\n",
    "    cls_batch_num: 10\n",
    "    output_format: [json]\n",
    "```\n",
    "\n",
    "### Override via CLI\n",
    "\n",
    "```bash\n",
    "# Use custom config files\n",
    "uv run scripts/run_ocr.py \\\n",
    "  --citekeys dagz_v01 \\\n",
    "  --output data/analytics/ocr \\\n",
    "  --pipeline-config my_pipeline.yaml \\\n",
    "  --predict-params my_predict.yaml\n",
    "```\n",
    "\n",
    "### Override via Environment Variables\n",
    "\n",
    "```bash\n",
    "# Custom B2 bucket\n",
    "export B2_SOURCE_BUCKET=\"my-custom-bucket\"\n",
    "\n",
    "# Custom batch size\n",
    "# (use CLI: --batch-pages 200)\n",
    "\n",
    "uv run scripts/run_ocr.py --citekeys dagz_v01 --output data/analytics/ocr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6edf493",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Error: Citekey NOT FOUND in B2\n",
    "\n",
    "**Problem:**\n",
    "```\n",
    "âœ— dagz_v01 - NOT FOUND in B2 (must be uploaded first via upload_pdfs_to_b2.py)\n",
    "```\n",
    "\n",
    "**Solution:** PDF hasn't been uploaded yet. Run `upload_pdfs.py` first:\n",
    "```bash\n",
    "uv run scripts/upload_pdfs.py --input data/sources/dagz_v01.pdf\n",
    "```\n",
    "\n",
    "### Error: Runpod endpoint not found\n",
    "\n",
    "**Problem:**\n",
    "```\n",
    "âœ— Batch 1 submission failed: 404 Client Error: Not Found for url: https://api.runpod.io/v2/4nq7w0tw2k8uwk/run\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "1. Check `RUNPOD_ENDPOINT_ID` is correct\n",
    "2. Verify endpoint is deployed and active on Runpod console\n",
    "3. Check `RUNPOD_API_KEY` is valid\n",
    "4. Test endpoint health:\n",
    "   ```bash\n",
    "   curl -X GET \"https://api.runpod.io/v2/4nq7w0tw2k8uwk/health\" \\\n",
    "     -H \"Authorization: Bearer $RUNPOD_API_KEY\"\n",
    "   ```\n",
    "\n",
    "### Error: B2 configuration incomplete\n",
    "\n",
    "**Problem:**\n",
    "```\n",
    "RuntimeError: B2 source configuration incomplete. Required: B2_SOURCE_ACCESS_KEY_ID, B2_SOURCE_SECRET_ACCESS_KEY\n",
    "```\n",
    "\n",
    "**Solution:** Set B2 credentials:\n",
    "```bash\n",
    "export B2_SOURCE_ACCESS_KEY_ID=\"your_key_id\"\n",
    "export B2_SOURCE_SECRET_ACCESS_KEY=\"your_secret\"\n",
    "```\n",
    "\n",
    "Or add to `.env` file (project root):\n",
    "```\n",
    "B2_SOURCE_ACCESS_KEY_ID=your_key_id\n",
    "B2_SOURCE_SECRET_ACCESS_KEY=your_secret\n",
    "```\n",
    "\n",
    "### Warning: Runpod S3 credentials incomplete\n",
    "\n",
    "**Problem:**\n",
    "```\n",
    "WARNING: Runpod S3 credentials incomplete. Image extraction from network volume will be skipped.\n",
    "```\n",
    "\n",
    "**Solution** (if you want image extraction):\n",
    "```bash\n",
    "export RUNPOD_S3_ACCESS_KEY_ID=\"user_xxxxx\"\n",
    "export RUNPOD_S3_SECRET_ACCESS_KEY=\"rps_xxxxx\"\n",
    "export RUNPOD_S3_ENDPOINT_URL=\"https://s3api-eu-ro-1.runpod.io\"\n",
    "export RUNPOD_S3_REGION=\"EU-RO-1\"\n",
    "export RUNPOD_NETWORK_VOLUME_ID=\"u2qv0e9yfd\"\n",
    "```\n",
    "\n",
    "Otherwise, ignore the warning. Image extraction is optional.\n",
    "\n",
    "### Error: No source job found for auto-detection\n",
    "\n",
    "**Problem:**\n",
    "```\n",
    "dagz_v01: (not found in source metadata)\n",
    "```\n",
    "\n",
    "**Solution:** Either:\n",
    "1. Re-run `upload_pdfs.py` with this citekey (creates metadata)\n",
    "2. Explicitly specify source job ID:\n",
    "   ```bash\n",
    "   uv run scripts/run_ocr.py --citekeys dagz_v01 --source-job-id 2026-01-01_20-48-14\n",
    "   ```\n",
    "3. Or use `--source-job-id latest` to use most recent upload batch\n",
    "\n",
    "### Error: PyPDF2 not installed\n",
    "\n",
    "**Problem:**\n",
    "```\n",
    "WARNING: Could not get page count for /path/to/pdf.pdf: name 'PyPDF2' is not defined, using estimate\n",
    "```\n",
    "\n",
    "**Solution** (optional):\n",
    "```bash\n",
    "uv add PyPDF2\n",
    "```\n",
    "\n",
    "Without PyPDF2, page counts are estimated from file size (accuracy depends on PDF compression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4a4c4b",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### For Data Workers\n",
    "\n",
    "1. **Always verify B2 upload first:**\n",
    "   ```bash\n",
    "   # Confirm PDFs are in B2 before running OCR\n",
    "   uv run scripts/upload_pdfs.py --input data/sources/ --dry-run\n",
    "   ```\n",
    "\n",
    "2. **Test with small batch:**\n",
    "   ```bash\n",
    "   # Process one PDF to verify setup before large batch\n",
    "   uv run scripts/run_ocr.py --citekeys test_v01 --output data/analytics/ocr\n",
    "   ```\n",
    "\n",
    "3. **Monitor Runpod costs:**\n",
    "   - GPU time is expensive\n",
    "   - Batch strategically (group similar-sized PDFs)\n",
    "   - Use reasonable `--batch-pages` values (300-500 is typical)\n",
    "\n",
    "4. **Document metadata:**\n",
    "   - Save which job ID you processed\n",
    "   - Note any configuration changes\n",
    "   - Link to source upload batch in notes\n",
    "\n",
    "### For Developers\n",
    "\n",
    "1. **Test locally first:**\n",
    "   ```bash\n",
    "   # Use test fixtures, not production data\n",
    "   uv run scripts/run_ocr.py --input tests/fixtures/sample_pdfs/ --output data/analytics/ocr\n",
    "   ```\n",
    "\n",
    "2. **Check results structure:**\n",
    "   ```bash\n",
    "   # Verify output format and metadata\n",
    "   cat data/analytics/ocr/<citekey>/latest/results.json | python -m json.tool\n",
    "   cat data/analytics/ocr/job_metadata/latest.json | python -m json.tool\n",
    "   ```\n",
    "\n",
    "3. **Monitor lineage integrity:**\n",
    "   - Verify `source_job_id` is set correctly\n",
    "   - Cross-check with source metadata\n",
    "   - Report gaps in lineage tracking\n",
    "\n",
    "4. **Tune batch size strategically:**\n",
    "   - Larger batches (500+ pages) = better GPU utilization but more memory\n",
    "   - Smaller batches (100-200 pages) = faster processing but less efficient\n",
    "   - Test with your endpoint's GPU spec\n",
    "\n",
    "### For the Pipeline\n",
    "\n",
    "1. **Metadata is your audit trail:**\n",
    "   - Don't delete `data/analytics/ocr/job_metadata/`\n",
    "   - Metadata enables reproducibility\n",
    "   - Use it to answer \"what was processed when?\"\n",
    "\n",
    "2. **B2 is authoritative:**\n",
    "   - Citekeys MUST exist in B2 before OCR\n",
    "   - Validation checks this automatically\n",
    "   - Prevents orphaned OCR results\n",
    "\n",
    "3. **Lineage links everything:**\n",
    "   - OCR job â†’ source upload job â†’ original PDFs\n",
    "   - Auto-detection reduces manual error\n",
    "   - Enables complete reproducibility\n",
    "\n",
    "4. **Results directory structure is standardized:**\n",
    "   ```\n",
    "   data/analytics/ocr/\n",
    "     <citekey>/\n",
    "       <job_id>/        # Results for this run\n",
    "         results.json\n",
    "         images/        # Optional visualization\n",
    "       latest -> <job_id>  # Easy access to newest\n",
    "     job_metadata/\n",
    "       <job_id>.json\n",
    "       latest.json -> <job_id>.json\n",
    "   ```\n",
    "   \n",
    "   Use this structure in downstream processing."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
