{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1160bb",
   "metadata": {},
   "source": [
    "## Directory Structure\n",
    "\n",
    "### Source Jobs (Uploads)\n",
    "\n",
    "```\n",
    "data/sources/job_metadata/\n",
    "  2026-01-01_20-48-14.json              # What was uploaded\n",
    "  latest.json -> 2026-01-01_20-48-14.json\n",
    "```\n",
    "\n",
    "### Analytics Jobs (Processing)\n",
    "\n",
    "```\n",
    "data/analytics/\n",
    "  \n",
    "  job_registry/                          # Central hub: full pipeline trace\n",
    "    2026-01-01_21-37-23.json            # Complete job record with all steps\n",
    "    latest.json -> 2026-01-01_...json\n",
    "  \n",
    "  ocr/\n",
    "    job_metadata/                        # OCR-specific metadata\n",
    "      2026-01-01_21-37-23.json          # OCR configs, results summary\n",
    "    citekey1/\n",
    "      2026-01-01_21-37-23/\n",
    "        citekey1.json                    # OCR results for this citekey\n",
    "    latest -> 2026-01-01_21-37-23\n",
    "  \n",
    "  toc/\n",
    "    job_metadata/\n",
    "      2026-01-01_21-37-24.json\n",
    "    citekey1/\n",
    "      2026-01-01_21-37-24/\n",
    "        citekey1_toc.json\n",
    "  \n",
    "  segmentation/\n",
    "    job_metadata/\n",
    "      2026-01-01_21-37-25.json\n",
    "    citekey1/\n",
    "      2026-01-01_21-37-25/\n",
    "        citekey1_segments.json\n",
    "```\n",
    "\n",
    "### File Naming\n",
    "\n",
    "- **`job_registry/`** = Central registry (hub) — what ran in the entire pipeline\n",
    "- **`{step}/job_metadata/`** = Task-specific metadata (spokes) — details for one step\n",
    "- **Same job ID everywhere** = Easy to link files across pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33094b60",
   "metadata": {},
   "source": [
    "## Central Registry Format\n",
    "\n",
    "The central registry (`data/analytics/job_registry/{job_id}.json`) contains the complete story of a job:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"job_id\": \"2026-01-01_21-37-23\",\n",
    "  \"timestamp\": \"2026-01-01T21:37:23Z\",\n",
    "  \n",
    "  \"source\": {\n",
    "    \"job_id\": \"2026-01-01_20-48-14\",\n",
    "    \"bucket\": \"cna-sources\",\n",
    "    \"citekeys\": {\n",
    "      \"total\": 50,\n",
    "      \"list\": [\"citekey1\", \"citekey2\", ...]\n",
    "    },\n",
    "    \"checksums\": {\n",
    "      \"citekey1\": \"abc123...\"\n",
    "    }\n",
    "  },\n",
    "  \n",
    "  \"pipeline_steps\": {\n",
    "    \"ocr\": {\n",
    "      \"job_id\": \"2026-01-01_21-37-23\",\n",
    "      \"status\": \"completed\",\n",
    "      \"timestamp\": \"2026-01-01T21:37:23Z\",\n",
    "      \"metadata_path\": \"ocr/job_metadata/2026-01-01_21-37-23.json\",\n",
    "      \"citekeys\": {\n",
    "        \"total\": 50,\n",
    "        \"successful\": 50,\n",
    "        \"failed\": 0,\n",
    "        \"list\": [...]\n",
    "      },\n",
    "      \"results_summary\": {\n",
    "        \"total_pages\": 5000,\n",
    "        \"total_images\": 0\n",
    "      }\n",
    "    },\n",
    "    \"toc\": {\n",
    "      \"job_id\": null,\n",
    "      \"status\": \"pending\"\n",
    "    },\n",
    "    \"segmentation\": {\n",
    "      \"job_id\": null,\n",
    "      \"status\": \"pending\"\n",
    "    }\n",
    "  },\n",
    "  \n",
    "  \"execution_trace\": [\n",
    "    {\n",
    "      \"step\": \"ocr\",\n",
    "      \"job_id\": \"2026-01-01_21-37-23\",\n",
    "      \"timestamp\": \"2026-01-01T21:37:23Z\",\n",
    "      \"status\": \"completed\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Key Fields:**\n",
    "\n",
    "- **`source`** — Links back to original upload (with checksums!)\n",
    "- **`pipeline_steps`** — Shows status of each planned step\n",
    "- **`execution_trace`** — Chronological record of what actually ran"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993620f8",
   "metadata": {},
   "source": [
    "## Task-Specific Metadata Format\n",
    "\n",
    "Each step's metadata (`data/analytics/{step}/job_metadata/{job_id}.json`) contains task-specific details:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"status\": \"completed\",\n",
    "  \"job_id\": \"2026-01-01_21-37-23\",\n",
    "  \"job_timestamp\": \"2026-01-01T21:37:23Z\",\n",
    "  \"pipeline_step\": \"ocr\",\n",
    "  \n",
    "  \"source_job_id\": \"2026-01-01_20-48-14\",\n",
    "  \n",
    "  \"citekeys\": {\n",
    "    \"total\": 50,\n",
    "    \"successful\": 50,\n",
    "    \"failed\": 0,\n",
    "    \"list\": [...]\n",
    "  },\n",
    "  \n",
    "  \"results_summary\": {\n",
    "    \"total_pages\": 5000,\n",
    "    \"total_images\": 0\n",
    "  },\n",
    "  \n",
    "  \"config\": {\n",
    "    \"pipeline_config\": { ... },\n",
    "    \"predict_params\": { ... }\n",
    "  },\n",
    "  \n",
    "  \"batching\": {\n",
    "    \"num_batches\": 2,\n",
    "    \"pages_per_batch\": 300\n",
    "  },\n",
    "  \n",
    "  \"runpod_jobs\": {\n",
    "    \"runpod_job_id_1\": [\"citekey1\", \"citekey2\"],\n",
    "    \"runpod_job_id_2\": [\"citekey3\", \"citekey4\"]\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211136f2",
   "metadata": {},
   "source": [
    "## Querying Jobs\n",
    "\n",
    "Use the `etl_metadata` module to query jobs programmatically.\n",
    "\n",
    "### List All Jobs\n",
    "\n",
    "```python\n",
    "from scripts.etl_metadata import list_all_jobs, get_latest_job\n",
    "\n",
    "# All jobs from central registry (all steps)\n",
    "all_jobs = list_all_jobs()\n",
    "# Output: [\"2026-01-01_21-37-23\", \"2026-01-01_20-48-14\", ...]\n",
    "\n",
    "# Jobs from a specific step only\n",
    "ocr_jobs = list_all_jobs(step_name=\"ocr\")\n",
    "# Output: [\"2026-01-01_21-37-23\", \"2026-01-01_20-20-00\", ...]\n",
    "\n",
    "# Get latest job\n",
    "latest = get_latest_job()\n",
    "# Output: \"2026-01-01_21-37-23\"\n",
    "\n",
    "latest_ocr = get_latest_job(step_name=\"ocr\")\n",
    "# Output: \"2026-01-01_21-37-23\"\n",
    "```\n",
    "\n",
    "### Get Job Details\n",
    "\n",
    "```python\n",
    "from scripts.etl_metadata import get_central_registry, get_step_metadata\n",
    "\n",
    "# Get full pipeline trace\n",
    "registry = get_central_registry(\"2026-01-01_21-37-23\")\n",
    "# Shows which steps completed, source job, execution trace\n",
    "\n",
    "# Get step-specific metadata\n",
    "ocr_metadata = get_step_metadata(\"ocr\", \"2026-01-01_21-37-23\")\n",
    "# Shows OCR configs, results summary, citekeys processed\n",
    "```\n",
    "\n",
    "### Check Pipeline Status\n",
    "\n",
    "```python\n",
    "from scripts.etl_metadata import get_job_pipeline_status\n",
    "\n",
    "status = get_job_pipeline_status(\"2026-01-01_21-37-23\")\n",
    "# Output:\n",
    "# {\n",
    "#   \"job_id\": \"2026-01-01_21-37-23\",\n",
    "#   \"timestamp\": \"2026-01-01T21:37:23Z\",\n",
    "#   \"steps_completed\": [\"ocr\"],\n",
    "#   \"steps_pending\": [\"toc\", \"segmentation\"],\n",
    "#   \"source_citekeys\": [\"citekey1\", \"citekey2\", ...],\n",
    "#   \"source_job_id\": \"2026-01-01_20-48-14\"\n",
    "# }\n",
    "```\n",
    "\n",
    "### Find Jobs for a Citekey\n",
    "\n",
    "```python\n",
    "from scripts.etl_metadata import find_jobs_for_citekey\n",
    "\n",
    "# Find all jobs that processed a specific citekey\n",
    "jobs = find_jobs_for_citekey(\"citekey1\")\n",
    "# Output:\n",
    "# [\n",
    "#   {\"job_id\": \"2026-01-01_20-48-14\", \"step\": \"source (upload)\", \"status\": \"completed\"},\n",
    "#   {\"job_id\": \"2026-01-01_21-37-23\", \"step\": \"ocr\", \"status\": \"completed\"},\n",
    "#   {\"job_id\": \"2026-01-01_21-37-24\", \"step\": \"toc\", \"status\": \"completed\"}\n",
    "# ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331444b8",
   "metadata": {},
   "source": [
    "## Common Workflows\n",
    "\n",
    "### Workflow 1: Check What Was Uploaded\n",
    "\n",
    "```python\n",
    "from scripts.etl_metadata import get_central_registry\n",
    "\n",
    "# Get a job's source information\n",
    "registry = get_central_registry(\"2026-01-01_21-37-23\")\n",
    "\n",
    "# See which PDFs were uploaded\n",
    "source = registry[\"source\"]\n",
    "print(f\"Uploaded from job: {source['job_id']}\")\n",
    "print(f\"Citekeys: {source['citekeys']['list']}\")\n",
    "print(f\"Bucket: {source['bucket']}\")\n",
    "\n",
    "# Check checksums to verify file integrity\n",
    "checksums = source['checksums']\n",
    "print(f\"SHA256 of citekey1: {checksums['citekey1']}\")\n",
    "```\n",
    "\n",
    "### Workflow 2: Verify OCR Completed Successfully\n",
    "\n",
    "```python\n",
    "from scripts.etl_metadata import get_job_pipeline_status\n",
    "\n",
    "status = get_job_pipeline_status(\"2026-01-01_21-37-23\")\n",
    "\n",
    "if \"ocr\" in status[\"steps_completed\"]:\n",
    "    print(\"✓ OCR completed\")\n",
    "else:\n",
    "    print(\"✗ OCR not yet completed\")\n",
    "\n",
    "# Get OCR results summary\n",
    "registry = get_central_registry(\"2026-01-01_21-37-23\")\n",
    "ocr_step = registry[\"pipeline_steps\"][\"ocr\"]\n",
    "print(f\"Pages processed: {ocr_step['results_summary']['total_pages']}\")\n",
    "print(f\"Successful citekeys: {ocr_step['citekeys']['successful']}\")\n",
    "```\n",
    "\n",
    "### Workflow 3: Trace a Citekey Through the Pipeline\n",
    "\n",
    "```python\n",
    "from scripts.etl_metadata import find_jobs_for_citekey\n",
    "\n",
    "citekey = \"citekey1\"\n",
    "jobs = find_jobs_for_citekey(citekey)\n",
    "\n",
    "print(f\"Complete lineage for {citekey}:\")\n",
    "for job in jobs:\n",
    "    print(f\"  {job['step']:<20} | Job: {job['job_id']} | Status: {job['status']}\")\n",
    "\n",
    "# Output:\n",
    "#   source (upload)      | Job: 2026-01-01_20-48-14 | Status: completed\n",
    "#   ocr                  | Job: 2026-01-01_21-37-23 | Status: completed\n",
    "#   toc                  | Job: 2026-01-01_21-37-24 | Status: completed\n",
    "```\n",
    "\n",
    "### Workflow 4: Find Latest OCR Job and Its Results\n",
    "\n",
    "```python\n",
    "from scripts.etl_metadata import get_latest_job, get_step_metadata\n",
    "from pathlib import Path\n",
    "\n",
    "# Get latest OCR job\n",
    "job_id = get_latest_job(step_name=\"ocr\")\n",
    "print(f\"Latest OCR job: {job_id}\")\n",
    "\n",
    "# Load OCR results\n",
    "metadata = get_step_metadata(\"ocr\", job_id)\n",
    "print(f\"Citekeys processed: {metadata['citekeys']['list']}\")\n",
    "print(f\"Total pages: {metadata['results_summary']['total_pages']}\")\n",
    "\n",
    "# Access OCR output files\n",
    "results_dir = Path(\"data/analytics/ocr\") / metadata['citekeys']['list'][0] / job_id\n",
    "print(f\"Results directory: {results_dir}\")\n",
    "```\n",
    "\n",
    "### Workflow 5: Rebuild Central Registry (if corrupted)\n",
    "\n",
    "```python\n",
    "from scripts.etl_metadata import rebuild_central_registry\n",
    "\n",
    "# Scan all step directories and rebuild central registry\n",
    "job_count = rebuild_central_registry()\n",
    "print(f\"Rebuilt registry with {job_count} jobs\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca78ca0",
   "metadata": {},
   "source": [
    "## Understanding Lineage\n",
    "\n",
    "### Complete Pipeline Trace\n",
    "\n",
    "```\n",
    "Source PDF (B2)\n",
    "        ↓\n",
    "   Upload Job (2026-01-01_20-48-14)\n",
    "        ↓ source_job_id\n",
    "   OCR Job (2026-01-01_21-37-23)\n",
    "        ↓\n",
    "   TOC Extraction Job (2026-01-01_21-37-24)\n",
    "        ↓\n",
    "   Segmentation Job (2026-01-01_21-37-25)\n",
    "```\n",
    "\n",
    "### How Lineage is Stored\n",
    "\n",
    "**Central Registry (`data/analytics/job_registry/2026-01-01_21-37-23.json`):**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"source\": {\n",
    "    \"job_id\": \"2026-01-01_20-48-14\",\n",
    "    \"citekeys\": { ... }\n",
    "  },\n",
    "  \"pipeline_steps\": {\n",
    "    \"ocr\": {\n",
    "      \"status\": \"completed\",\n",
    "      \"citekeys\": { ... }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "One file shows:\n",
    "- ✅ Which PDFs were uploaded (from source)\n",
    "- ✅ Which PDFs were processed by OCR\n",
    "- ✅ Which PDFs will be processed by next steps\n",
    "- ✅ Configuration used at each step\n",
    "- ✅ Timestamps of when each step ran\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "**Reproducibility**: \"Exactly which PDF was used to produce this OCR?\"\n",
    "→ Check source job ID and checksums\n",
    "\n",
    "**Debugging**: \"If OCR output is wrong, where did the PDF come from?\"\n",
    "→ Follow the source_job_id back to the upload\n",
    "\n",
    "**Audit Trail**: \"What changed between OCR runs?\"\n",
    "→ Compare configs in two job_registry files\n",
    "\n",
    "**Data Integrity**: \"Have these files been tampered with?\"\n",
    "→ Compare stored checksums with B2 files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701998a8",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### For Data Workers\n",
    "\n",
    "1. **Always check the central registry** to understand what happened:\n",
    "   ```bash\n",
    "   cat data/analytics/job_registry/latest.json | python -m json.tool\n",
    "   ```\n",
    "\n",
    "2. **Note job IDs when you run pipelines**:\n",
    "   ```\n",
    "   Uploaded on 2026-01-01 with job: 2026-01-01_20-48-14\n",
    "   Processed OCR on 2026-01-01 with job: 2026-01-01_21-37-23\n",
    "   ```\n",
    "\n",
    "3. **Use job tracking for documentation**:\n",
    "   - Document which job processed which batch\n",
    "   - Note any issues and their job IDs\n",
    "   - Keep job IDs in processing notes\n",
    "\n",
    "### For Developers\n",
    "\n",
    "1. **Check job registry for debugging**:\n",
    "   - See which steps have completed\n",
    "   - Compare configs between different job runs\n",
    "   - Identify bottlenecks in the pipeline\n",
    "\n",
    "2. **Use metadata for monitoring**:\n",
    "   ```python\n",
    "   from scripts.etl_metadata import get_job_pipeline_status\n",
    "   \n",
    "   # Build a monitoring dashboard\n",
    "   status = get_job_pipeline_status(latest_job_id)\n",
    "   print(f\"Completed: {len(status['steps_completed'])} steps\")\n",
    "   print(f\"Pending: {len(status['steps_pending'])} steps\")\n",
    "   ```\n",
    "\n",
    "3. **Never delete job metadata**:\n",
    "   - It's your audit trail\n",
    "   - Enables reproducibility\n",
    "   - Costs nothing to keep\n",
    "\n",
    "### For the Pipeline\n",
    "\n",
    "1. **Lineage is automatic** — each step automatically links to source\n",
    "2. **Central registry is self-contained** — no need to cross-reference files\n",
    "3. **Checksums enable verification** — compare stored vs actual file hashes\n",
    "4. **Timestamps enable ordering** — understand the sequence of operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381c303d",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Job Registry Entry Missing\n",
    "\n",
    "**Problem**: Can't find `data/analytics/job_registry/{job_id}.json`\n",
    "\n",
    "**Possible Causes**:\n",
    "1. Job ID is from a source upload (check `data/sources/job_metadata/` instead)\n",
    "2\n",
    ",\n",
    "3\n",
    ",\n",
    ",\n",
    "\n",
    "### Central Registry Out of Sync\n",
    "\n",
    "**Problem**: Central registry is missing entries from step metadata directories\n",
    "\n",
    "**Solution**: Rebuild the registry\n",
    "```python\n",
    "from scripts.etl_metadata import rebuild_central_registry\n",
    "rebuild_central_registry()\n",
    "```\n",
    "\n",
    "### Can't Find Citekey in Pipeline\n",
    "\n",
    "**Problem**: Can't trace a citekey through OCR and TOC steps\n",
    "\n",
    "**Solution**: Use the lineage finder\n",
    "```python\n",
    "from scripts.etl_metadata import find_jobs_for_citekey\n",
    "jobs = find_jobs_for_citekey(\"my_citekey\")\n",
    "print(jobs)\n",
    "```\n",
    "\n",
    "This will show all jobs that touched your citekey, including the source upload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc7fcb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Aspect | Details |\n",
    "|--------|----------|\n",
    "| **Central Registry** | `data/analytics/job_registry/{job_id}.json` — Single source of truth |\n",
    "| **Task Metadata** | `data/analytics/{step}/job_metadata/{job_id}.json` — Step-specific details |\n",
    "| **Job ID** | Timestamp format: `YYYY-MM-DD_HH-MM-SS` — Globally unique |\n",
    "| **Lineage** | Source → Upload → OCR → TOC → Segmentation |\n",
    "| **Key Link** | `source_job_id` in metadata links analytics jobs back to uploads |\n",
    "| **Query API** | Use `etl_metadata` module for programmatic access |\n",
    "| **Reproducibility** | Checksums + timestamps enable exact recreation of any job |\n",
    "\n",
    "**Bottom Line**: Every job generates a complete audit trail. Use job IDs to trace data through the entire pipeline and verify that everything ran correctly."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
