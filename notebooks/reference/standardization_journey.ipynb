{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c959f826",
   "metadata": {},
   "source": [
    "## 1. The Problem\n",
    "\n",
    "### Initial State (December 2025)\n",
    "\n",
    "The ETL pipeline had evolved organically over time, resulting in inconsistent interfaces across steps:\n",
    "\n",
    "#### upload_pdfs.py\n",
    "```bash\n",
    "python scripts/upload_pdfs.py --input config/docs.yaml\n",
    "```\n",
    "- ‚úÖ Simple and clear\n",
    "- ‚ùå No citekey filtering support\n",
    "\n",
    "#### run_ocr.py\n",
    "```bash\n",
    "python scripts/run_ocr.py /path/to/pdfs/\n",
    "python scripts/run_ocr.py --citekeys dag_v01 dag_v02\n",
    "```\n",
    "- ‚úÖ Flexible (PDFs or citekeys)\n",
    "- ‚ùå No source job tracking\n",
    "- ‚ùå Ambiguous when both provided\n",
    "\n",
    "#### sync_ocr.py\n",
    "```bash\n",
    "python scripts/sync_ocr.py --ocr-job-id 2026-01-02_00-33-11\n",
    "python scripts/sync_ocr.py --ocr-job-id latest\n",
    "```\n",
    "- ‚úÖ Clear source reference\n",
    "- ‚ùå \"latest\" breaks reproducibility\n",
    "- ‚ùå No citekey filtering\n",
    "\n",
    "#### parse_structure.py\n",
    "```bash\n",
    "python scripts/parse_structure.py --ocr-job-id 2026-01-02_00-33-11\n",
    "python scripts/parse_structure.py --sync-job-id 2026-01-02_11-15-30\n",
    "python scripts/parse_structure.py --citekeys dag_v01\n",
    "```\n",
    "- ‚ùå Multiple job ID options confusing\n",
    "- ‚ùå Unclear which to use when\n",
    "- ‚ùå No clear lineage chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1042ff",
   "metadata": {},
   "source": [
    "### Pain Points\n",
    "\n",
    "#### 1. Reproducibility Issues\n",
    "```bash\n",
    "# Run 1 (January 2)\n",
    "python scripts/sync_ocr.py --ocr-job-id latest\n",
    "# ‚Üí Uses job 2026-01-02_00-33-11\n",
    "\n",
    "# Run 2 (January 3) \n",
    "python scripts/sync_ocr.py --ocr-job-id latest\n",
    "# ‚Üí Now uses job 2026-01-03_08-15-00\n",
    "# üö® Same command, different results!\n",
    "```\n",
    "\n",
    "#### 2. Lost Lineage\n",
    "```bash\n",
    "# Which upload job did this OCR come from?\n",
    "python scripts/run_ocr.py --citekeys dagz_v01\n",
    "# No way to track back to upload_pdfs job!\n",
    "```\n",
    "\n",
    "#### 3. No Standard Way to Resume\n",
    "```bash\n",
    "# Job 2026-01-02_11-32-58 had 3 failures\n",
    "# How do I resume just the failures?\n",
    "# ‚Üí No built-in support, must manually identify failed citekeys\n",
    "```\n",
    "\n",
    "#### 4. Ambiguous Parameters\n",
    "```bash\n",
    "# parse_structure.py accepts both\n",
    "python scripts/parse_structure.py \\\n",
    "    --ocr-job-id 2026-01-02_00-33-11 \\\n",
    "    --sync-job-id 2026-01-02_11-15-30\n",
    "# Which one takes precedence? Not obvious!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbdef5c",
   "metadata": {},
   "source": [
    "## 2. Design Iterations\n",
    "\n",
    "### Iteration 1: \"Just Add Source Tracking\"\n",
    "\n",
    "Initial thought: Just add `--upload-job-id` to run_ocr, `--ocr-job-id` to sync_ocr, etc.\n",
    "\n",
    "```bash\n",
    "python scripts/run_ocr.py \\\n",
    "    --upload-job-id 2026-01-02_00-28-28 \\\n",
    "    --citekeys dagz_v01\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "- Still inconsistent (some steps use paths, some use citekeys)\n",
    "- Doesn't solve resume problem\n",
    "- Different parameter names per step (upload-job-id, ocr-job-id, sync-job-id)\n",
    "\n",
    "**Verdict:** ‚ùå Not enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe39a6e",
   "metadata": {},
   "source": [
    "### Iteration 2: \"Unified Input Pattern\"\n",
    "\n",
    "Insight: Every step needs the same three ways to specify input:\n",
    "1. From a file (`--input`)\n",
    "2. Explicit list (`--citekeys`)\n",
    "3. Resume failures (`--resume-from`)\n",
    "\n",
    "```bash\n",
    "# Standard pattern\n",
    "python scripts/{step}.py \\\n",
    "    [--input FILE | --citekeys CK1 CK2 ... | --resume-from JOB_ID] \\\n",
    "    --source-job-id JOB_ID\n",
    "```\n",
    "\n",
    "**Improvements:**\n",
    "- ‚úÖ Consistent across all steps\n",
    "- ‚úÖ Built-in resume support\n",
    "- ‚úÖ Clear lineage with unified `--source-job-id`\n",
    "\n",
    "**Remaining issues:**\n",
    "- What about \"latest\"?\n",
    "- How to handle reprocessing?\n",
    "\n",
    "**Verdict:** ‚úÖ Getting closer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226283f",
   "metadata": {},
   "source": [
    "### Iteration 3: \"Ban 'latest' and Add Force Rerun\"\n",
    "\n",
    "Key decisions:\n",
    "\n",
    "#### Decision 1: No More \"latest\"\n",
    "```python\n",
    "def validate_job_id(job_id: str, arg_name: str):\n",
    "    if job_id == \"latest\":\n",
    "        raise ValueError(\n",
    "            f\"{arg_name} cannot be 'latest' (breaks reproducibility).\\n\"\n",
    "            f\"Use explicit timestamp like: 2026-01-02_00-33-11\"\n",
    "        )\n",
    "```\n",
    "- Symlinks still useful for reading\n",
    "- But never as pipeline inputs\n",
    "\n",
    "#### Decision 2: Add Force Rerun Flag\n",
    "```bash\n",
    "python scripts/parse_structure.py \\\n",
    "    --source-job-id 2026-01-02_11-15-30 \\\n",
    "    --citekeys dagz_v01 \\\n",
    "    --force-rerun\n",
    "```\n",
    "- Always creates new job\n",
    "- Processes all citekeys (ignores existing results)\n",
    "- Use when parameters change\n",
    "\n",
    "**Verdict:** ‚úÖ Solid foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef57f41f",
   "metadata": {},
   "source": [
    "### Iteration 4: \"Add Power Features\"\n",
    "\n",
    "With the core standardization in place, add convenience features:\n",
    "\n",
    "#### Wildcard Support (Limited)\n",
    "```bash\n",
    "# Process all volumes\n",
    "python scripts/parse_structure.py \\\n",
    "    --source-job-id 2026-01-02_11-15-30 \\\n",
    "    --citekeys \"dagz_v*\"\n",
    "```\n",
    "- Only support `_v*` (volumes) and `_y*` (years)\n",
    "- Prevents accidental broad matches\n",
    "\n",
    "#### Dry Run\n",
    "```bash\n",
    "# Preview before executing\n",
    "python scripts/parse_structure.py \\\n",
    "    --source-job-id 2026-01-02_11-15-30 \\\n",
    "    --citekeys \"dagz_v*\" \\\n",
    "    --dry-run\n",
    "```\n",
    "- Shows what would be processed\n",
    "- Displays skip reasons\n",
    "- No execution\n",
    "\n",
    "**Verdict:** ‚úÖ Complete solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83caae8a",
   "metadata": {},
   "source": [
    "## 3. Final Solution\n",
    "\n",
    "### Standardized Interface\n",
    "\n",
    "All pipeline steps (except upload_pdfs) now follow this pattern:\n",
    "\n",
    "```bash\n",
    "python scripts/{step}.py \\\n",
    "    [--input FILE | --citekeys CK1 CK2 ... | --resume-from JOB_ID] \\\n",
    "    --source-job-id EXPLICIT_TIMESTAMP \\\n",
    "    [--force-rerun] \\\n",
    "    [--dry-run]\n",
    "```\n",
    "\n",
    "### Key Principles\n",
    "\n",
    "1. **Explicit over implicit:** No \"latest\", always use timestamps\n",
    "2. **Single source of truth:** Each step references only immediate prior step\n",
    "3. **Mutually exclusive inputs:** Can't mix --input, --citekeys, --resume-from\n",
    "4. **Required lineage:** --source-job-id always required (except upload_pdfs)\n",
    "5. **Safe defaults:** Without --force-rerun, skip existing results\n",
    "6. **Limited wildcards:** Only _v* and _y* to prevent errors\n",
    "\n",
    "### Run Mode Semantics\n",
    "\n",
    "| Mode | Creates New Job? | Processes |\n",
    "|------|-----------------|----------|\n",
    "| Regular run | If needed | Missing results only |\n",
    "| Resume | No (reuses) | Failed items only |\n",
    "| Force rerun | Always | All items |\n",
    "| Dry run | Never | Nothing (preview only) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d643fb83",
   "metadata": {},
   "source": [
    "## 4. Implementation\n",
    "\n",
    "### Core Utilities (etl_metadata.py)\n",
    "\n",
    "#### Validation\n",
    "```python\n",
    "def validate_job_id(job_id: str, arg_name: str):\n",
    "    \"\"\"Reject 'latest', validate timestamp format.\"\"\"\n",
    "    if job_id == \"latest\":\n",
    "        raise ValueError(\n",
    "            f\"{arg_name} cannot be 'latest' (breaks reproducibility).\\n\"\n",
    "            f\"Use explicit timestamp like: 2026-01-02_00-33-11\"\n",
    "        )\n",
    "    # Validate format: YYYY-MM-DD_HH-MM-SS\n",
    "    pattern = r\"^\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2}$\"\n",
    "    if not re.match(pattern, job_id):\n",
    "        raise ValueError(\n",
    "            f\"{arg_name} must be in format YYYY-MM-DD_HH-MM-SS, got: {job_id}\"\n",
    "        )\n",
    "```\n",
    "\n",
    "#### Resume Support\n",
    "```python\n",
    "def get_failed_citekeys(\n",
    "    step_name: str, \n",
    "    resume_job_id: str, \n",
    "    source_job_id: str\n",
    ") -> List[str]:\n",
    "    \"\"\"Extract failed citekeys from previous job.\"\"\"\n",
    "    metadata_file = ANALYTICS_ROOT / step_name / \"job_metadata\" / f\"{resume_job_id}.json\"\n",
    "    \n",
    "    with metadata_file.open(\"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Verify same source\n",
    "    if metadata.get(\"source_job_id\") != source_job_id:\n",
    "        raise ValueError(\"Source job ID mismatch\")\n",
    "    \n",
    "    # Extract failed items\n",
    "    failed = [\n",
    "        ck for ck, info in metadata[\"citekeys\"].items()\n",
    "        if info.get(\"status\") == \"failed\"\n",
    "    ]\n",
    "    return failed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3f1f6",
   "metadata": {},
   "source": [
    "#### Force Rerun Logic\n",
    "```python\n",
    "def should_process_citekey(\n",
    "    step_name: str,\n",
    "    citekey: str,\n",
    "    source_job_id: str,\n",
    "    force_rerun: bool = False,\n",
    "    output_dir: Optional[Path] = None\n",
    ") -> Tuple[bool, str]:\n",
    "    \"\"\"Determine if citekey should be processed.\"\"\"\n",
    "    \n",
    "    if force_rerun:\n",
    "        return True, \"Force rerun requested\"\n",
    "    \n",
    "    # Check if result exists for this citekey + source_job_id\n",
    "    result_file = output_dir / citekey / \"latest\" / f\"{citekey}.json\"\n",
    "    if result_file.exists():\n",
    "        with result_file.open(\"r\") as f:\n",
    "            result = json.load(f)\n",
    "        if result.get(\"source_job_id\") == source_job_id:\n",
    "            return False, f\"Result exists\"\n",
    "    \n",
    "    return True, \"No matching result\"\n",
    "```\n",
    "\n",
    "#### Wildcard Expansion\n",
    "```python\n",
    "def expand_citekey_patterns(\n",
    "    patterns: List[str],\n",
    "    available_citekeys: List[str]\n",
    ") -> List[str]:\n",
    "    \"\"\"Expand _v* and _y* patterns only.\"\"\"\n",
    "    \n",
    "    expanded = set()\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        if \"*\" in pattern:\n",
    "            # Validate: only _v* or _y* allowed\n",
    "            if not (\"_v*\" in pattern or \"_y*\" in pattern):\n",
    "                raise ValueError(\n",
    "                    f\"Only _v* and _y* wildcards supported, got: {pattern}\"\n",
    "                )\n",
    "            matches = fnmatch.filter(available_citekeys, pattern)\n",
    "            expanded.update(matches)\n",
    "        else:\n",
    "            expanded.add(pattern)\n",
    "    \n",
    "    return sorted(expanded)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac3ace",
   "metadata": {},
   "source": [
    "#### Dry Run Preview\n",
    "```python\n",
    "def preview_pipeline_run(\n",
    "    step_name: str,\n",
    "    citekeys: List[str],\n",
    "    source_job_id: str,\n",
    "    force_rerun: bool = False\n",
    ") -> Dict:\n",
    "    \"\"\"Generate dry run preview.\"\"\"\n",
    "    \n",
    "    # Filter what would be processed\n",
    "    to_process, skip_reasons = filter_citekeys_to_process(\n",
    "        step_name, citekeys, source_job_id, force_rerun\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"step_name\": step_name,\n",
    "        \"source_job_id\": source_job_id,\n",
    "        \"total_citekeys\": len(citekeys),\n",
    "        \"to_process\": to_process,\n",
    "        \"to_skip\": skip_reasons,\n",
    "        \"force_rerun\": force_rerun,\n",
    "        \"will_create_new_job\": len(to_process) > 0\n",
    "    }\n",
    "\n",
    "def print_dry_run_summary(preview: Dict):\n",
    "    \"\"\"Pretty print preview.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üîç DRY RUN PREVIEW: {preview['step_name']}\")\n",
    "    print(\"=\"*70)\n",
    "    # ... detailed output ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95936f0",
   "metadata": {},
   "source": [
    "### Per-Step Changes\n",
    "\n",
    "#### upload_pdfs.py\n",
    "```python\n",
    "# Before\n",
    "parser.add_argument(\"--input\", required=True)\n",
    "\n",
    "# After\n",
    "input_group = parser.add_mutually_exclusive_group(required=True)\n",
    "input_group.add_argument(\"--input\")\n",
    "input_group.add_argument(\"--citekeys\", nargs=\"+\")\n",
    "# Note: No --source-job-id (it's the source!)\n",
    "```\n",
    "\n",
    "#### run_ocr.py\n",
    "```python\n",
    "# Before\n",
    "parser.add_argument(\"pdf_dir\", nargs=\"?\")\n",
    "parser.add_argument(\"--citekeys\", nargs=\"*\")\n",
    "\n",
    "# After\n",
    "input_group = parser.add_mutually_exclusive_group(required=True)\n",
    "input_group.add_argument(\"--input\")\n",
    "input_group.add_argument(\"--citekeys\", nargs=\"+\")\n",
    "input_group.add_argument(\"--resume-from\")\n",
    "parser.add_argument(\"--source-job-id\", required=True)  # Upload job\n",
    "parser.add_argument(\"--force-rerun\", action=\"store_true\")\n",
    "parser.add_argument(\"--dry-run\", action=\"store_true\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30c318",
   "metadata": {},
   "source": [
    "#### sync_ocr.py\n",
    "```python\n",
    "# Before\n",
    "parser.add_argument(\"--ocr-job-id\", required=True)\n",
    "# Supported --ocr-job-id latest\n",
    "\n",
    "# After\n",
    "input_group = parser.add_mutually_exclusive_group(required=True)\n",
    "input_group.add_argument(\"--input\")\n",
    "input_group.add_argument(\"--citekeys\", nargs=\"+\")\n",
    "input_group.add_argument(\"--resume-from\")\n",
    "parser.add_argument(\"--source-job-id\", required=True)  # OCR job\n",
    "parser.add_argument(\"--force-rerun\", action=\"store_true\")\n",
    "parser.add_argument(\"--dry-run\", action=\"store_true\")\n",
    "\n",
    "# Removed resolve_job_id() function entirely\n",
    "```\n",
    "\n",
    "#### parse_structure.py\n",
    "```python\n",
    "# Before\n",
    "parser.add_argument(\"--ocr-job-id\")\n",
    "parser.add_argument(\"--sync-job-id\")\n",
    "parser.add_argument(\"--citekeys\", nargs=\"*\")\n",
    "\n",
    "# After\n",
    "input_group = parser.add_mutually_exclusive_group(required=True)\n",
    "input_group.add_argument(\"--input\")\n",
    "input_group.add_argument(\"--citekeys\", nargs=\"+\")\n",
    "input_group.add_argument(\"--resume-from\")\n",
    "parser.add_argument(\"--source-job-id\", required=True)  # Sync job only!\n",
    "parser.add_argument(\"--force-rerun\", action=\"store_true\")\n",
    "parser.add_argument(\"--dry-run\", action=\"store_true\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78618738",
   "metadata": {},
   "source": [
    "## 5. Lessons Learned\n",
    "\n",
    "### Design Principles\n",
    "\n",
    "1. **Consistency is king**\n",
    "   - Users develop muscle memory\n",
    "   - Easier to document and teach\n",
    "   - Reduces cognitive load\n",
    "\n",
    "2. **Explicit over implicit**\n",
    "   - \"latest\" seemed convenient but caused confusion\n",
    "   - Explicit timestamps make everything traceable\n",
    "   - Verbosity is better than ambiguity\n",
    "\n",
    "3. **Single responsibility per parameter**\n",
    "   - Don't mix concerns (--ocr-job-id vs --sync-job-id)\n",
    "   - Use uniform names (--source-job-id everywhere)\n",
    "   - Clear what each parameter controls\n",
    "\n",
    "4. **Build for the 99% case**\n",
    "   - Most runs are regular processing\n",
    "   - Make common case easy, advanced cases possible\n",
    "   - Default to safe behavior (skip existing results)\n",
    "\n",
    "5. **Preview before execution**\n",
    "   - Dry run prevents costly mistakes\n",
    "   - Shows what *would* happen\n",
    "   - Builds confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068a19d",
   "metadata": {},
   "source": [
    "### Technical Decisions\n",
    "\n",
    "#### Why Mutually Exclusive Input Groups?\n",
    "```python\n",
    "input_group = parser.add_mutually_exclusive_group(required=True)\n",
    "input_group.add_argument(\"--input\")\n",
    "input_group.add_argument(\"--citekeys\", nargs=\"+\")\n",
    "input_group.add_argument(\"--resume-from\")\n",
    "```\n",
    "- Prevents ambiguous combinations\n",
    "- argparse enforces exactly one\n",
    "- Clear error messages\n",
    "\n",
    "#### Why Ban \"latest\"?\n",
    "- Same command ‚Üí different results over time\n",
    "- Hard to debug \"why did this change?\"\n",
    "- Symlinks still useful for reading, just not pipeline inputs\n",
    "\n",
    "#### Why Limit Wildcards?\n",
    "- `*` or `dag*` could match hundreds of files\n",
    "- `_v*` and `_y*` match our naming conventions\n",
    "- Prevents accidental bulk operations\n",
    "\n",
    "#### Why Separate --source-job-id from --resume-from?\n",
    "```bash\n",
    "# Resume failed items from job X, but process against source Y\n",
    "python scripts/parse_structure.py \\\n",
    "    --source-job-id 2026-01-02_11-15-30 \\  # Sync job (data source)\n",
    "    --resume-from 2026-01-02_11-32-58     # Parse job (previous attempt)\n",
    "```\n",
    "- Different purposes: data lineage vs execution history\n",
    "- Resume validates source matches\n",
    "- Prevents accidentally mixing data versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2368281",
   "metadata": {},
   "source": [
    "### Migration Strategy\n",
    "\n",
    "How we rolled out the changes:\n",
    "\n",
    "1. **Phase 1: Add utilities** (etl_metadata.py)\n",
    "   - validate_job_id()\n",
    "   - get_failed_citekeys()\n",
    "   - No breaking changes yet\n",
    "\n",
    "2. **Phase 2: Update one step** (run_ocr.py)\n",
    "   - Full refactor to new interface\n",
    "   - Test thoroughly\n",
    "   - Document changes\n",
    "\n",
    "3. **Phase 3: Roll out to remaining steps**\n",
    "   - sync_ocr.py\n",
    "   - parse_structure.py\n",
    "   - upload_pdfs.py (minor changes)\n",
    "\n",
    "4. **Phase 4: Add advanced features**\n",
    "   - Wildcard support\n",
    "   - Dry run preview\n",
    "   - Force rerun logic\n",
    "\n",
    "5. **Phase 5: Document**\n",
    "   - Pipeline overview notebook\n",
    "   - This design journal\n",
    "   - Update README"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe1c71a",
   "metadata": {},
   "source": [
    "### What We'd Do Differently\n",
    "\n",
    "Looking back, here's what we'd change:\n",
    "\n",
    "1. **Start with standardization**\n",
    "   - Should have designed interface first\n",
    "   - Then implemented steps\n",
    "   - Harder to refactor than build right initially\n",
    "\n",
    "2. **Document as we go**\n",
    "   - Easy to forget design rationale\n",
    "   - This notebook should have been written during design\n",
    "   - Future-you will thank past-you\n",
    "\n",
    "3. **Add validation earlier**\n",
    "   - \"latest\" caused problems for months\n",
    "   - Simple validation would have caught issues\n",
    "   - Fail fast, fail loud\n",
    "\n",
    "4. **Test with real users**\n",
    "   - Our abstractions made sense to us\n",
    "   - Users had different mental models\n",
    "   - Dry run came from user feedback\n",
    "\n",
    "### What Worked Well\n",
    "\n",
    "1. **Incremental refactoring**\n",
    "   - Didn't break everything at once\n",
    "   - Could test each step independently\n",
    "   - Maintained working system throughout\n",
    "\n",
    "2. **Utility functions**\n",
    "   - Centralized validation\n",
    "   - Reusable across steps\n",
    "   - Single source of truth\n",
    "\n",
    "3. **Notebooks for documentation**\n",
    "   - Runnable examples\n",
    "   - Can show actual data\n",
    "   - More engaging than plain docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6b8902",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The standardization effort took significant time but delivered:\n",
    "\n",
    "- ‚úÖ **Consistent interface** across all pipeline steps\n",
    "- ‚úÖ **Reproducible results** (no more \"latest\")\n",
    "- ‚úÖ **Clear lineage** (source_job_id chain)\n",
    "- ‚úÖ **Robust resume** (built into all steps)\n",
    "- ‚úÖ **Safe operations** (dry run, validation)\n",
    "- ‚úÖ **Power features** (wildcards, force rerun)\n",
    "\n",
    "Most importantly: **Users can now predict how any pipeline step works** based on knowing one step.\n",
    "\n",
    "That's the real win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e8877f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
